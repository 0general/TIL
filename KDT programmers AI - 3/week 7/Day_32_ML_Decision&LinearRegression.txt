
□ 결정이론 (Decision Theory)

■ 결정이론

● 새로운 값 x가 주어졌을 때 확률모델 p(x,t)에 기반해 최적의 결정(예를 들어 분류)을 내리는 것

  - 추론단계 : 결합확률분포 p(x, Ck)를 구하는 것( p(Ck|x) 를 직접 구하는 경우도 있음 ). 이것만 있으면 모든 것을 할 수 있음
  - 결정단계 : 상황에 대한 확률이 주어졌을 때 어떻게 최적의 결정을 내릴 것인지? 추론단계를 거쳤다면 결정단계는 매우 쉬움.

■ 결정이론 - 이진분류 ( Binary Classification )

 - 결정영역 (decision region)

   Ri = { x : pred(x) = Ci}

 - 분류오류 확률 (probability of misclassification)

   p(mis) = p(x ∈ R₁, C₂) + p(x ∈ R₂, C₁)
               x가 R₁에 속해있을 때 C₂로 판단, x가 R₂에 속해있을 때 C₁으로 판단
               (원래는 C₁이지만 C₂로 인식하는 경우 + 원래는 C₂이지만 C₁으로 인식하는 경우)
          = ∫(R₁) p(x, C₂)dx + ∫(R₂) p(x, C₁)dx

 - 오류를 최소화하려면 다음 조건을 만족하면 x를 R₁에 할당해야한다.

   p(x, C₁) > p(x, C₂)
    ⇔ p(C₁|x)p(x) > p(C₂|x)p(x)
    ⇔ p(C₁|x) > p(C₂|x)

● Multicalss일 경우

 클래스가 여러 개일 경우는, mis를 생각하는 것보다 얼마나 정확한가를 생각하는 것이 낫다.

  p(correct) = ∑(k=1, K) p(x ∈ Rk, Ck)
             = ∑(k=1, K) ∫(Rk) p(x, Ck)dx 

● 결정이론의 목표 (분류의 경우)

 결합확률분포 p(x, Ck)가 주어졌을 때 최적의 결정영역들 R₁, ..., Rk를 찾는 것

 최적의 함수를 찾자.


■ 기대손실 최소화 (Minimizing the Expected loss)

● 모든 결정이 동일한 리스크를 갖는 것은 아님.

  - 암이 아닌데 암인 것으로 진단
  - 암이 맞는데 암이 아닌 것으로 진단

  위 두 경우는 동일한 리스크를 가진다고 할 수 없다.
  두 번째 경우가 훨씬 더 큰 리스크를 가진다.

● 손실행렬 (loss matrix)
 
 - Lkj : Ck에 속하는 x를 Cj로 분류할 때 발생하는 손실(또는 비용)

 데이터에 대한 모든 지식이 확률분포로 표현되고 있는 것을 기억할 것. 
 한 데이터샘플 x의 실제 클래스를 결정론적으로 알고 있는 것이 아니라 그것의 확률만을 알 수 있다고 가정한다.
 즉, 우리가 관찰할 수 있는 샘플(예를 들어, 암을 가진 환자의 X-Ray 이미지)은 확률분포 p(x,Ck)를 통해서 생성된 것이라고 간주한다.

 따라서 손실행렬 L이 주어졌을 때, 다음과 같은 기대손실을 최소화하는 것을 목표로 할 수 있다.

                    E[L] = ∑(k)∑(j) ∫(Rj) Lkj * p(x, Ck)dx
   


■ 결정이론 - 회귀문제

● 목표값  t ∈ R

● 손실함수 : L(t, y(x)) = {y(x)-t}²

 범함수(functional, 함수의 함수)의 최소값을 구하는 문제


● 손실함수의 분해

  {y(x)-t}² = {y(x) - E[t|x] + E[t|x] - t}²
            = {y(x) - E[t|x]}² + 2{y(x) - E[t|x]}{E[t|x] - t} + {E[t|x] - t}²

  교차항(←  2{y(x) - E[t|x]}{E[t|x] - t}  ) cross term은 아래와 같이 사라진다.

          2 ∬ {y(x) - E[t|x]}{E[t|x] - t}p(x,t)dxdt
           = 2 ∬ {y(x) - E[t|x]}{E[t|x] - t}p(t|x)p(x)dxdt
           = 2 ∫ {y(x) - E[t|x]}p(x)( ∫ {E[t|x] - t}p(t|x)dt )dx
           = 2 ∫ {y(x) - E[t|x]}p(x)( E[t|x]*∫ p(t|x)dt - ∫ t*p(t|x)dt )dx
           = 2 ∫ {y(x) - E[t|x]}p(x)(E[t|x]-E[t|x])dx
           = 0
  따라서,

            E[L] = ∫ {y(x) - E[t|x]}²p(x)dx + ∫ var[t|x]p(x)dx

  - 함수 y(x)는 첫번째 항에만 나타나며 그 값이 E[t|x]와 동일할 때 기대손실이 최소화됨을 알 수 있다.
  - 두번째 항은 t의 분산(x에 대해 평균화된)이며 함수 y(x)와 상관없다. 따라서 이 값은 더 이상 줄일 수 없는 기대손실의 최소값이다.


■ 결정 문제를 위한 몇 가지 방법들

● 분류문제의 경우

 - 확률모델에 의존하는 경우
   + 생성모델(generative model): 먼저 각 클래스 Ck에 대해 분포 p(x|Ck)와 사전확률 p(Ck)를 구한 다음
                                 베이즈 정리를 사용해서 사후확률 p(Ck|x)를 구한다.
        
        사후확률이 주어졌기 때문에 분류를 위한 결정은 쉽게 이루어질 수 있다. 
        결합분포에서 데이터 샘플링해서 "생성"할 수 있으므로 이런 방식을 생성모델이라고 부른다.

   + 식별모델(discriminative model): 모든 분포를 다 계산하지 않고 오직 사후확률 p(Ck|x)를 구한다.
                                    위와 동일하게 결정이론을 적용할 수 있다.

                                    조금은 복잡하다는 단점이 있다. 

 - 판별함수에 의존하는 경우
   + 판별함수 (discriminant function): 입력 x을 클래스로 할당하는 판별함수(discriminant function)를 찾는다. 
                                       확률값은 계산하지 않는다.


● 회귀문제의 경우

 - 결합분포 p(x,t)를 구하는 추론(inference)문제를 먼저 푼 다음 조건부확률분포 p(t|x)를 구한다.
   그리고 주변화(marginalize)를 통해 Et[t|x]를 구한다.

 - 조건부확률분포 p(t|x)를 구하는 추론문제를 푼 다음 주변화(marginalize)를 통해 Et[t|x]를 구한다.

 - y(x)를 직접 구한다.




________________________________________________________________________________________________________________________

선형회귀 문제 실습!
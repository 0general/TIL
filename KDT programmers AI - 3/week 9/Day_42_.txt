◆ 딥러닝 기초
□ 인공신경망과 생물신경망

■ 사람의 뉴런 neuron
  - 두뇌의 가장 작은 정보처리 단위
  - 구조
    * 세포체 cell body는 간단한 연산
    * 수상돌기 dendrite 는 신호 수신
    * 축삭 axon 은 처리 결과를 전송
  - 사람은 10¹¹ 개 정도의 뉴런을 가지면, 각 뉴런은 약 1000개 다른 뉴런과 연결되어 10¹⁴ 개 연결을 가짐

■ 두 줄기 연구의 동반상승 synergy 효과
  - 컴퓨터 과학 computer science 
    + 컴퓨터의 계산(연산) 능력의 획기적 발전

  - 뇌(의학) 과학 발전

|__ 사람 신경망 __|__ 인공 신경망 __|
|     세포체      |      노드      |
|    수상돌기     |      입력      |
|      축삭       |      출력      |
|     시냅스      |     가중치     |

가중치 : 뉴런이 연결세기를 결정했던 것과 같은 역할


□ 신경망의 종류

■ 전방 신경망과 순환 신경망
■ 얕은 신경망과 깊은 신경망
■ 결정론deterministic 신경망과 확률론적stochastic 신경망 비교
 - 결정론 신경망 : 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망
 - 확률론 신경망 : 


__________________________________________________________________________________________________________

◆ 퍼셉트론

■ 구조 : 절 node, 가중치 weight, 층 layer과 같은 새로운 개념의 구조 도입

■ 제시된 퍼셉트론 구조의 학습 알고리즘을 제안

■ 원시적 신경망이지만, 깊은 인공신경망을 포함한 현대 인공신경망의 토대

 - 깊은 인공신경망은 퍼셉트론의 병렬 배치를 순차적으로 구조로 결합함

→ 현대 인공신경망 구조에 아주 중요

■ 퍼셉트론의 구조
 - 입력
   * i번째 노드는 특징 벡터 x = (x1, x2, ... , xd)^T 의 요소 xi를 담당
   * 항상 1이 입력되는 편향 bias 노드 포함

 - 입력과 출력 사이에 연산하는 구조를 가짐
   * i번째 입력 노드와 출력 노드를 연결하는 변 edge는 가중치 wi를 가짐
   * 퍼셉트론은 단일 층 구조라고 간주함

 - 출력
   * 한 개의 노드에 의해 수치(+1 혹은 -1) 출력 

■ 퍼셉트론의 동작

 - 선형 연산 → 비선형 연산
   * [선형] 입력(특징)갑솨 가중치를 곱하고 모두 더해 s를 구함
   * [비선형] 활성함수 τ를 적용
     + 활성함수 τ로 계단 함수 step function를 사용 → 출력 y = +1 또는 y = -1

 - 수식
   y = τ(s)

   이 때  s = w0 + ∑(i=1,d)wi*xi         τ(s) = 1 if s >= 0 else -1
               ↖ 편향, (= w0*x0 , x0 = 1)


□ 동작

■ 행렬 표기 matrix vector notation

■ 기하학적 의미
 
  - 결정 직선 d(x) = d(x1,x2) = w1x1 + w2x2 + w0 = 0 
    * w1과 w2는 직선의 기울기, w0는 절편 intercept(편향)을 결정

    * 결정 직선은 특징 공간을 +1 과 -1의 두 부분공간으로 이분항하는 분류기 역할

  - d차원 공간으로 일반화 d(x) = w1x1 + w2x2 + ... +wdxd + w0 = 0

    * 2차원 : 결정 직선 decision line, 3차원 : 결정 평면 decision plane, 4차원 이상 : 결정 초평면 decision hyperplane

□ 학습

■ 일반적인 분류기의 학습 과정

  - 단계 1 : 과업 정의와 분류 과정의 수학적 정의 (가설 설정)
  - 단계 2 : 해당 분류기의 목적함수 J(Θ) 정의
  - 단계 3 : J(Θ)를 최소화하는 Θ를 찾기 위한 최적화 방법 수행

■ 목적함수 정의 (단계 1 + 단계 2)

  - 퍼셉트론(가설 혹은 모델 설정)의 매개변수를 w = (w0, w1, ..., wd)^T라 표기하면
    매개변수 집합은 Θ = {w} 표기

  - 목적함수 J(Θ) 또는 J(w)로 표기

  - 퍼셉트론 목적함수의 상세 조건

    *[조건 1] :  J(w) ≥ 0 이다.

    *[조건 2] :  w가 최적이면, 즉 모든 샘플을 맞히면 J(w) = 0 이다.

    *[조건 3] :  틀리는 샘플이 많은 w일수록 J(w)는 큰 값을 가진다.



■ 목적함수 상세 설계

  J(w) = ∑(xk ∈ Y) -yk(w^T * xk)
            ← Y는 w가 틀리는 샘플의 집합 (오분류 집합)

  - [조건 1] : 임의의 샘플 xk가 Y에 속한다면, 퍼셉트론의 예측 값 w^T * xk 와 실제 값 yk는 부호가 다름
        → -yk(w^T * xk)는 항상 양수를 가짐 : 만족

  - [조건 2] : 결국 Y가 클수록(틀린 샘플이 많을수록), J(w)는 큰 값ㅇ르 가짐 : 만족

  - [조건 3] : Y가 공집합일 때 (즉, 퍼셉트론이 모든 샘플을 맞출 때), J(w) = 0임 : 만족


■ 경사 하강법 gradient descent (3단계)
  - 최소 J(Θ) 기울기를 이용하여 반복 탐색하여 극값을 찾음

■ 학습률 learning rate의 중요성
◆ 다층 퍼셉트론

퍼셉트론 : 사람의 뇌를 모방한 신경망의 첫번째 신경 모델
선형 분류만 가능하다는 한계를 가지고 있었다.

■ 퍼셉트론 : 선형 분류기 linear classifier 한계
  선형 분리가 불가능한 상황에서 일정한 양의 오류를 가진다.

■ 다층 퍼셉트론의 핵심 아이디어

  - 은닉층을 둔다.
    은닉층 : 원래 특징 공간을 분류하는 데 훨씬 유리한 새로운 특징 공간으로 변환한다.

  - 시그모이드 활성함수를 도입한다.
    퍼셉트론은 계단함수를 활성함수로 사용했다. 계단함수는 경성 hard 의사결정에 해당한다.
    반면에, 다층 퍼셉트론은 연성 soft 의사결정이 가능한 시그모이드함수를 활성함수로 사용한다. 
    연성에서는 출력이 연속값인데, 출력을 신뢰도로 간주함으로써 더 융통성 있게 의사결정을 할 수 있다.

  - 오류 역전파 알고리즘을 사용한다.
    다층 퍼셉트론은 여러 층이 순차적으로 이어진 구조이므로, 
    역방향으로 진행하면서 한 번에 한 층씩 그레이디언트를 계산하고 가중치를 갱신하는 방식의 오류 역전파 알고리즘을 사용한다.

□ 특징 공간 변환

■ 퍼셉트론 2개를 사용한 XOR 문제의 해결

  and 문제나 or문제는 퍼셉트론 하나로 가능한다.
  exclusive-or는 nand와 or로 구현이 가능하므로, 퍼셉트론의 여러 조합을 이용하여 xor를 구현할 수 있지 않을까? → 그러하다.

  - 퍼셉트론 1과 퍼셉트론 2가 모두 동일한지 그렇지 않은지로 구별 가능

■ 퍼셉트론 2개를 병렬 결합하면, 원래 공간 x = (x1,x2)^T 를 새로운 특징 공간 z = (z1,z2)^T로 변환 → 새로운 특징 공간 z에서 선형 분리 가능함

  사람이 수작업 특징 학습 hand-craft features learning을 수행한 것과 유사하다. ← 표현 학습


■ 다층 퍼셉트론의 용량capacity

  p개 퍼셉트론을 결합하면 p차원 공간으로 변환

  - 3개 퍼셉트론을 결합하면 2차원 공간으로 7개 영역으로 나누고 각 영역을 3차원 점으로 변환한다. 계단함수를 활성함수 τ로 사용을 가정하였으므로 영역을 점으로 변환



□ 활성함수

■ 딱딱한 공간 분할과 부드러운 공간 분할

  - 계단함수는 딱딱한 의사결정 → 영역을 점으로 변환

  - 그외 활성함수는 부드러운 의사결정 → 영역을 영역으로 변환

■ 대표적인 비선형 함수인 S자 모양의 sigmoid 를 활성함수로 사용

  이진 시그모이드 :  0~1
  양극 시그모이드 : -1~1


■ 신경망이 사용하는 다양한 활성함수

 - 범위도 잘 알아두도록 하자


■ 일반적으로 은닉층에서 logistic sigmoid를 활성 함수로 많이 사용

  - S자 모양의 넓은 포화곡선은 경사도 기반한 학습(오류 역전파)을 어렵게 한다. → 깊은 신경망에서는 ReLU(rectifier) 활용

